---
layout:     post
title:      "如何使用local-pv"
#subtitle:   "本文翻译自istio官方文档"
description: "本文演示使用local-pv作为redis-cluster存储"
excerpt: "本文演示使用local-pv作为redis-cluster存储"
date:     2021-12-01
#author:     ""
image: "https://unsplash.com/blog/content/images/2021/04/screen-01-2.jpg"
categories: [ "Tech"]
tags:
- Kubertes  
URL: "/2021/12/01/local-pv/"
---

# 使用local-volume作为redis-cluster存储

## Local Persistent Volume与Hostpath区别

- 对于HostPath卷，调度程序可能会将引用HostPath卷的pod移至其他节点，从而导致数据丢失。但是对于本地持久卷，Kubernetes调度程序可确保始终将使用本地持久卷的容器调度到同一节点。
- statefulset 无法使用hostpath作为存储

## 基于local-pvc创建好pv、pvc、storageclass

创建6个不同的pv、pvc，这里给出一个模板

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-0-data
  labels:
    alicloud-pvname: redis-0-data
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete  # 表示绑定的pvc删除时，动态配置的pv一并删除，所以手动创建的pv不会删除，需要手动删除
  storageClassName: local-volume
  local:
    path: /disk1/redis-0-data # 宿主机目录，需要手动创建
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master-1  # 亲和性，表示pod只能在该节点启动
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: redis-cluster-data-redis-cluster-0  # pvc名称严格按照volumename-statefulsetname-序号
  namespace: redis
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: local-volume # storageclass name
  selector:
    matchLabels:
      alicloud-pvname: redis-0-data
```

创建storageclass，local-volume的storageclass 不支持动态创建pv，该storageclass只实现了延迟绑定的功能

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-volume  # pv、pvc配置该name
provisioner: kubernetes.io/no-provisioner # local-volume不支持动态创建pv
volumeBindingMode: WaitForFirstConsumer # 延迟绑定pv，等pod在该节点上启动再绑定pv
```

## 创建redis-cluster

其他配置不用变，只需改变pvc模板

```yaml
volumeClaimTemplates:  #PVC模板
  - metadata:
      name: redis-cluster-data
    spec:
      accessModes: [ "ReadWriteOnce"]
      storageClassName: local-volume  # 只需更改该name
      resources:
        requests:
          storage: 2Gi
```

## 验证

```yaml
# 发现pod按照顺序启动在对应的local-pv的节点上
[root@master-1 local-pv]# kubectl get pods -n redis -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
redis-cluster-0   1/1     Running   0          19m   10.233.106.157   master-1   <none>           <none>
redis-cluster-1   1/1     Running   0          19m   10.233.109.133   master-2   <none>           <none>
redis-cluster-2   1/1     Running   0          19m   10.233.113.126   master-3   <none>           <none>
redis-cluster-3   1/1     Running   0          19m   10.233.112.155   node-1     <none>           <none>
redis-cluster-4   1/1     Running   0          19m   10.233.112.156   node-1     <none>           <none>
redis-cluster-5   1/1     Running   0          19m   10.233.113.127   master-3   <none>           <none>
```

重启一个pod，发现该pod依然在该节点启动

```yaml
[root@master-1 local-pv]# kubectl delete pods redis-cluster-0  -n redis
pod "redis-cluster-0" deleted
[root@master-1 local-pv]# kubectl get pods -n redis -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
redis-cluster-0   1/1     Running   0          27s   10.233.106.158   master-1   <none>           <none>
redis-cluster-1   1/1     Running   0          21m   10.233.109.133   master-2   <none>           <none>
redis-cluster-2   1/1     Running   0          21m   10.233.113.126   master-3   <none>           <none>
redis-cluster-3   1/1     Running   0          21m   10.233.112.155   node-1     <none>           <none>
redis-cluster-4   1/1     Running   0          21m   10.233.112.156   node-1     <none>           <none>
redis-cluster-5   1/1     Running   0          21m   10.233.113.127   master-3   <none>           <none>
```

## 删除集群

```yaml
kubectl delete -f cm.yaml
kubectl delete -f statefulset.yaml  # 虽然该yaml配置了volumeClaimTemplates，但是不会删除pvc，需要手动删除pvc
kubectl get pvc -n redis |awk '{print $1}'|xargs -n 1 kubectl delete pvc -n redis # 删除pvc也不会删除pv，需要手动删除pv
kubectl delete pv redis-0-data  # 这里删除了pv，并不会删除宿主机目录的数据，需要手动删除
[root@master-1 redis-0-data]# pwd
/disk1/redis-0-data
[root@master-1 redis-0-data]# rm -rf data/

```

## local-pv调度pod的流程

正常pod调度时，pvc绑定pv与pod调度是同时进行的，所以有可能在pod调度到节点之前，pvc已经绑定到适合的pv了，但是使用local-pv，会导致一个问题。如果pod需要的数据在node-1上，但是该pod申明的pvc在集群里找到node-1和node-2两个pv都满足要求，现在pvc与node-2上pv绑定了，那么pod调度到node-2上会导致使用的是node-2的数据，这样肯定会有问题。所以使用延迟绑定实现pvc与pv绑定延迟到pod调度到节点之后，就能保证pod使用的数据是正确的，具体调度过程如下：

- 调度器执行预选算法函数，遍历node是否满足pv的节点分布情况
- 如果第一次发布，那么这时候statefulset申明的pvc还没有绑定pv，这时候需要延迟绑定，遍历集群里未绑定的pv，其NodeAffinity是否匹配该遍历循环的node，如满足，记录这个pvc与pv的映射关系缓存到bindingInfo中，留给最终绑定使用。

[Untitled](https://www.notion.so/0d5d2c31f7a44b94857b32b6dff98e5d)

       上表即可表示bindingInfo记录的信息，statefulset创建的pod-1就知道调度到node-1上了，因为pod-1声明的pvc是按照volumename-statefulsetname-num创建的，即pvc-1，所以这样pod即可调度到正确的节点上了。

- 如果是pod发生了重启，这时候调度器会重新调度该pod，但是该pod的pvc还是不变，依然能寻找到对应的node
    
    
    第一次发布时，pvc是随机绑定的pv，后续跟这个pvc绑定pod会一直在这个pv所在节点运行
    
    延迟绑定的使用场景，是pod已经申明了需要在哪个节点运行的情况下，这时候不延迟绑定pv、pvc的话，会导致pod使用的数据不对